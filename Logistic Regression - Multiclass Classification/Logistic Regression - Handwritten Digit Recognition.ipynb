{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('Digit_Data.csv')\n",
    "\n",
    "x1 = dataset['x1'].tolist()\n",
    "x2 = dataset['x2'].tolist()\n",
    "x3 = dataset['x3'].tolist()\n",
    "x4 = dataset['x4'].tolist()\n",
    "x5 = dataset['x5'].tolist()\n",
    "x6 = dataset['x6'].tolist()\n",
    "x7 = dataset['x7'].tolist()\n",
    "x8 = dataset['x8'].tolist()\n",
    "x9 = dataset['x9'].tolist()\n",
    "x10 = dataset['x10'].tolist()\n",
    "x11 = dataset['x11'].tolist()\n",
    "x12 = dataset['x12'].tolist()\n",
    "x13 = dataset['x13'].tolist()\n",
    "x14 = dataset['x14'].tolist()\n",
    "x15 = dataset['x15'].tolist()\n",
    "x16 = dataset['x16'].tolist()\n",
    "x17 = dataset['x17'].tolist()\n",
    "x18 = dataset['x18'].tolist()\n",
    "x19 = dataset['x19'].tolist()\n",
    "x20 = dataset['x20'].tolist()\n",
    "x21 = dataset['x21'].tolist()\n",
    "x22 = dataset['x22'].tolist()\n",
    "x23 = dataset['x23'].tolist()\n",
    "x24 = dataset['x24'].tolist()\n",
    "x25 = dataset['x25'].tolist()\n",
    "x26 = dataset['x26'].tolist()\n",
    "x27 = dataset['x27'].tolist()\n",
    "x28 = dataset['x28'].tolist()\n",
    "x29 = dataset['x29'].tolist()\n",
    "x30 = dataset['x30'].tolist()\n",
    "x31 = dataset['x31'].tolist()\n",
    "x32 = dataset['x32'].tolist()\n",
    "x33 = dataset['x33'].tolist()\n",
    "x34 = dataset['x34'].tolist()\n",
    "x35 = dataset['x35'].tolist()\n",
    "x36 = dataset['x36'].tolist()\n",
    "x37 = dataset['x37'].tolist()\n",
    "x38 = dataset['x38'].tolist()\n",
    "x39 = dataset['x39'].tolist()\n",
    "x40 = dataset['x40'].tolist()\n",
    "x41 = dataset['x41'].tolist()\n",
    "x42 = dataset['x42'].tolist()\n",
    "x43 = dataset['x43'].tolist()\n",
    "x44 = dataset['x44'].tolist()\n",
    "x45 = dataset['x45'].tolist()\n",
    "x46 = dataset['x46'].tolist()\n",
    "x47 = dataset['x47'].tolist()\n",
    "x48 = dataset['x48'].tolist()\n",
    "x49 = dataset['x49'].tolist()\n",
    "x50 = dataset['x50'].tolist()\n",
    "x51 = dataset['x51'].tolist()\n",
    "x52 = dataset['x52'].tolist()\n",
    "x53 = dataset['x53'].tolist()\n",
    "x54 = dataset['x54'].tolist()\n",
    "x55 = dataset['x55'].tolist()\n",
    "x56 = dataset['x56'].tolist()\n",
    "x57 = dataset['x57'].tolist()\n",
    "x58 = dataset['x58'].tolist()\n",
    "x59 = dataset['x59'].tolist()\n",
    "x60 = dataset['x60'].tolist()\n",
    "x61 = dataset['x61'].tolist()\n",
    "x62 = dataset['x62'].tolist()\n",
    "x63 = dataset['x63'].tolist()\n",
    "x64 = dataset['x64'].tolist()\n",
    "y   = dataset['y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  ...  x56  x57  x58  x59  x60  x61  \\\n",
       "0   0   0   5  13   9   1   0   0   0    0  ...    0    0    0    6   13   10   \n",
       "1   0   0   0  12  13   5   0   0   0    0  ...    0    0    0    0   11   16   \n",
       "2   0   0   0   4  15  12   0   0   0    0  ...    0    0    0    0    3   11   \n",
       "\n",
       "   x62  x63  x64  y  \n",
       "0    0    0    0  0  \n",
       "1   10    0    0  1  \n",
       "2   16    9    0  2  \n",
       "\n",
       "[3 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(dataset['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples=m=dataset['x1'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is multiclass classification problem with 10 number of classes\n"
     ]
    }
   ],
   "source": [
    "unique_classes=np.unique(outputs)\n",
    "\n",
    "if(len(unique_classes)>2):\n",
    "    print(\" This is multiclass classification problem with {} number of classes\".format(len(unique_classes)))\n",
    "else:\n",
    "    print(\"This is binary classification problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(dataset):\n",
    "    x1 = dataset['x1'].tolist()\n",
    "    x2 = dataset['x2'].tolist()\n",
    "    x3 = dataset['x3'].tolist()\n",
    "    x4 = dataset['x4'].tolist()\n",
    "    x5 = dataset['x5'].tolist()\n",
    "    x6 = dataset['x6'].tolist()\n",
    "    x7 = dataset['x7'].tolist()\n",
    "    x8 = dataset['x8'].tolist()\n",
    "    x9 = dataset['x9'].tolist()\n",
    "    x10 = dataset['x10'].tolist()\n",
    "    x11 = dataset['x11'].tolist()\n",
    "    x12 = dataset['x12'].tolist()\n",
    "    x13 = dataset['x13'].tolist()\n",
    "    x14 = dataset['x14'].tolist()\n",
    "    x15 = dataset['x15'].tolist()\n",
    "    x16 = dataset['x16'].tolist()\n",
    "    x17 = dataset['x17'].tolist()\n",
    "    x18 = dataset['x18'].tolist()\n",
    "    x19 = dataset['x19'].tolist()\n",
    "    x20 = dataset['x20'].tolist()\n",
    "    x21 = dataset['x21'].tolist()\n",
    "    x22 = dataset['x22'].tolist()\n",
    "    x23 = dataset['x23'].tolist()\n",
    "    x24 = dataset['x24'].tolist()\n",
    "    x25 = dataset['x25'].tolist()\n",
    "    x26 = dataset['x26'].tolist()\n",
    "    x27 = dataset['x27'].tolist()\n",
    "    x28 = dataset['x28'].tolist()\n",
    "    x29 = dataset['x29'].tolist()\n",
    "    x30 = dataset['x30'].tolist()\n",
    "    x31 = dataset['x31'].tolist()\n",
    "    x32 = dataset['x32'].tolist()\n",
    "    x33 = dataset['x33'].tolist()\n",
    "    x34 = dataset['x34'].tolist()\n",
    "    x35 = dataset['x35'].tolist()\n",
    "    x36 = dataset['x36'].tolist()\n",
    "    x37 = dataset['x37'].tolist()\n",
    "    x38 = dataset['x38'].tolist()\n",
    "    x39 = dataset['x39'].tolist()\n",
    "    x40 = dataset['x40'].tolist()\n",
    "    x41 = dataset['x41'].tolist()\n",
    "    x42 = dataset['x42'].tolist()\n",
    "    x43 = dataset['x43'].tolist()\n",
    "    x44 = dataset['x44'].tolist()\n",
    "    x45 = dataset['x45'].tolist()\n",
    "    x46 = dataset['x46'].tolist()\n",
    "    x47 = dataset['x47'].tolist()\n",
    "    x48 = dataset['x48'].tolist()\n",
    "    x49 = dataset['x49'].tolist()\n",
    "    x50 = dataset['x50'].tolist()\n",
    "    x51 = dataset['x51'].tolist()\n",
    "    x52 = dataset['x52'].tolist()\n",
    "    x53 = dataset['x53'].tolist()\n",
    "    x54 = dataset['x54'].tolist()\n",
    "    x55 = dataset['x55'].tolist()\n",
    "    x56 = dataset['x56'].tolist()\n",
    "    x57 = dataset['x57'].tolist()\n",
    "    x58 = dataset['x58'].tolist()\n",
    "    x59 = dataset['x59'].tolist()\n",
    "    x60 = dataset['x60'].tolist()\n",
    "    x61 = dataset['x61'].tolist()\n",
    "    x62 = dataset['x62'].tolist()\n",
    "    x63 = dataset['x63'].tolist()\n",
    "    x64 = dataset['x64'].tolist()\n",
    "    \n",
    "    y  = dataset['y_modified'].tolist()\n",
    "    x0 = [1]* len(x1)\n",
    "    \n",
    "    # Parameters\n",
    "    no_of_samples = m = len(x1)\n",
    "    no_of_features = 64\n",
    "    no_of_weights = no_of_features + 1\n",
    "    learning_rate = 0.008\n",
    "\n",
    "    input_vector = np.array([x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24,x25,x26,x27,x28,x29,x30,x31,x32,x33,x34,x35,x36,x37,x38,x39,x40,x41,x42,x43,x44,x45,x46,x47,x48,x49,x50,x51,x52,x53,x54,x55,x56,x57,x58,x59,x60,x61,x62,x63,x64])\n",
    "    actual_output_vector = np.array([y])\n",
    "    cost_vector=[]\n",
    "    sample_weight_vector=[]\n",
    "    weight_vector = np.array([1]*65).reshape(1,65)\n",
    "    prediction_vector_intermediate = weight_vector.dot(input_vector)\n",
    "\n",
    "    # Apply sigmoid function\n",
    "    prediction_vector=[]\n",
    "    for each_value in prediction_vector_intermediate.T:\n",
    "        predicted_value=1/(1+(np.exp(-each_value)))\n",
    "        prediction_vector.append(predicted_value)\n",
    "    prediction_vector=np.array(prediction_vector).reshape(1,m)\n",
    "\n",
    "    error_vector = np.subtract(prediction_vector, actual_output_vector)\n",
    "\n",
    "    sum_squared_error = 0\n",
    "    for error_value in error_vector.T:\n",
    "        sum_squared_error = sum_squared_error + (error_value*error_value)\n",
    "    cost = (1/m) * (1/2) * (sum_squared_error)\n",
    "    print(\"Cost = \",cost)\n",
    "    previous_cost=cost\n",
    "\n",
    "    gradient_matrix = error_vector.dot(input_vector.T)\n",
    "    multiplication_factor = (learning_rate/m)\n",
    "    weight_vector = np.subtract(weight_vector, multiplication_factor*gradient_matrix)\n",
    "    count=0\n",
    "    while True:\n",
    "        count=count+1\n",
    "        prediction_vector_intermediate = weight_vector.dot(input_vector)\n",
    "\n",
    "        # Apply sigmoid function\n",
    "        prediction_vector=[]\n",
    "        for each_value in prediction_vector_intermediate.T:\n",
    "            predicted_value=1/(1+(np.exp(-each_value)))\n",
    "            prediction_vector.append(predicted_value)\n",
    "        prediction_vector=np.array(prediction_vector).reshape(1,m)\n",
    "        error_vector = np.subtract(prediction_vector, actual_output_vector)\n",
    "\n",
    "        sum_squared_error = 0\n",
    "        for error_value in error_vector.T:\n",
    "            sum_squared_error = sum_squared_error + (error_value*error_value)\n",
    "        cost = (1/m) * (1/2) * (sum_squared_error)\n",
    "        if count==300:\n",
    "            print(\"cost = \",cost)\n",
    "            count=0\n",
    "    \n",
    "        cost_vector.append(cost)\n",
    "        sample_weight_vector.append(weight_vector[0][2])\n",
    "    \n",
    "        if cost>previous_cost or cost<0.012:\n",
    "            print(\"Final Cost=\",previous_cost)\n",
    "            break\n",
    "\n",
    "        gradient_matrix = error_vector.dot(input_vector.T)\n",
    "        multiplication_factor = (learning_rate/m)\n",
    "        weight_vector = np.subtract(weight_vector, multiplication_factor*gradient_matrix)\n",
    "        previous_cost=cost\n",
    "    # Check the loss function\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(sample_weight_vector,cost_vector)\n",
    "    plt.xlabel(\"weight\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 0 is in progress\n",
      "Cost =  [0.45047301]\n",
      "cost =  [0.01276981]\n",
      "Final Cost= [0.0120042]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 0 is done\n",
      "Training for class 1 is in progress\n",
      "Cost =  [0.44936004]\n",
      "cost =  [0.0324199]\n",
      "cost =  [0.02355255]\n",
      "cost =  [0.01863785]\n",
      "cost =  [0.01548835]\n",
      "cost =  [0.01322366]\n",
      "Final Cost= [0.01200229]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 1 is done\n",
      "Training for class 2 is in progress\n",
      "Cost =  [0.45075125]\n",
      "cost =  [0.01832305]\n",
      "Final Cost= [0.01200009]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 2 is done\n",
      "Training for class 3 is in progress\n",
      "Cost =  [0.4490818]\n",
      "cost =  [0.02442101]\n",
      "cost =  [0.0150774]\n",
      "Final Cost= [0.01200643]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 3 is done\n",
      "Training for class 4 is in progress\n",
      "Cost =  [0.44963829]\n",
      "Final Cost= [0.01200866]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 4 is done\n",
      "Training for class 5 is in progress\n",
      "Cost =  [0.44936004]\n",
      "cost =  [0.02009788]\n",
      "cost =  [0.01225685]\n",
      "Final Cost= [0.01201133]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 5 is done\n",
      "Training for class 6 is in progress\n",
      "Cost =  [0.44963829]\n",
      "cost =  [0.01729094]\n",
      "Final Cost= [0.01201032]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 6 is done\n",
      "Training for class 7 is in progress\n",
      "Cost =  [0.45019477]\n",
      "cost =  [0.01490902]\n",
      "Final Cost= [0.01201592]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 7 is done\n",
      "Training for class 8 is in progress\n",
      "Cost =  [0.45158598]\n",
      "cost =  [0.04139335]\n",
      "cost =  [0.02715412]\n",
      "cost =  [0.02076492]\n",
      "cost =  [0.01813979]\n",
      "cost =  [0.01689142]\n",
      "cost =  [0.01619336]\n",
      "cost =  [0.01571678]\n",
      "cost =  [0.01532136]\n",
      "cost =  [0.0149779]\n",
      "cost =  [0.01468183]\n",
      "cost =  [0.01443025]\n",
      "cost =  [0.01421902]\n",
      "cost =  [0.01404314]\n",
      "cost =  [0.01389707]\n",
      "cost =  [0.01377516]\n",
      "cost =  [0.01367223]\n",
      "cost =  [0.01358392]\n",
      "cost =  [0.01350684]\n",
      "cost =  [0.01343844]\n",
      "cost =  [0.0133769]\n",
      "cost =  [0.0133209]\n",
      "cost =  [0.01326951]\n",
      "cost =  [0.01322203]\n",
      "cost =  [0.01317797]\n",
      "cost =  [0.01313693]\n",
      "cost =  [0.01309859]\n",
      "cost =  [0.01306271]\n",
      "cost =  [0.01302906]\n",
      "cost =  [0.01299747]\n",
      "cost =  [0.01296777]\n",
      "cost =  [0.01293981]\n",
      "cost =  [0.01291345]\n",
      "cost =  [0.01288858]\n",
      "cost =  [0.01286508]\n",
      "cost =  [0.01284286]\n",
      "cost =  [0.01282182]\n",
      "cost =  [0.01280187]\n",
      "cost =  [0.01278293]\n",
      "cost =  [0.01276493]\n",
      "cost =  [0.01274781]\n",
      "cost =  [0.01273149]\n",
      "cost =  [0.01271593]\n",
      "cost =  [0.01270107]\n",
      "cost =  [0.01268687]\n",
      "cost =  [0.01267327]\n",
      "cost =  [0.01266024]\n",
      "cost =  [0.01264774]\n",
      "cost =  [0.01263574]\n",
      "cost =  [0.0126242]\n",
      "cost =  [0.01261309]\n",
      "cost =  [0.01260239]\n",
      "cost =  [0.01259207]\n",
      "cost =  [0.01258211]\n",
      "cost =  [0.01257249]\n",
      "cost =  [0.01256318]\n",
      "cost =  [0.01255418]\n",
      "cost =  [0.01254545]\n",
      "cost =  [0.01253699]\n",
      "cost =  [0.01252879]\n",
      "cost =  [0.01252082]\n",
      "cost =  [0.01251308]\n",
      "cost =  [0.01250555]\n",
      "cost =  [0.01249822]\n",
      "cost =  [0.01249108]\n",
      "cost =  [0.01248413]\n",
      "cost =  [0.01247735]\n",
      "cost =  [0.01247074]\n",
      "cost =  [0.01246428]\n",
      "cost =  [0.01245797]\n",
      "cost =  [0.01245181]\n",
      "cost =  [0.01244578]\n",
      "cost =  [0.01243989]\n",
      "cost =  [0.01243412]\n",
      "cost =  [0.01242846]\n",
      "cost =  [0.01242292]\n",
      "cost =  [0.01241749]\n",
      "cost =  [0.01241217]\n",
      "cost =  [0.01240694]\n",
      "cost =  [0.01240181]\n",
      "cost =  [0.01239678]\n",
      "cost =  [0.01239183]\n",
      "cost =  [0.01238696]\n",
      "cost =  [0.01238218]\n",
      "cost =  [0.01237748]\n",
      "cost =  [0.01237285]\n",
      "cost =  [0.01236829]\n",
      "cost =  [0.01236381]\n",
      "cost =  [0.01235939]\n",
      "cost =  [0.01235504]\n",
      "cost =  [0.01235075]\n",
      "cost =  [0.01234652]\n",
      "cost =  [0.01234234]\n",
      "cost =  [0.01233823]\n",
      "cost =  [0.01233416]\n",
      "cost =  [0.01233015]\n",
      "cost =  [0.01232619]\n",
      "cost =  [0.01232228]\n",
      "cost =  [0.01231842]\n",
      "cost =  [0.0123146]\n",
      "cost =  [0.01231082]\n",
      "cost =  [0.01230709]\n",
      "cost =  [0.0123034]\n",
      "cost =  [0.01229975]\n",
      "cost =  [0.01229613]\n",
      "cost =  [0.01229256]\n",
      "cost =  [0.01228902]\n",
      "cost =  [0.01228551]\n",
      "cost =  [0.01228204]\n",
      "cost =  [0.0122786]\n",
      "cost =  [0.0122752]\n",
      "cost =  [0.01227182]\n",
      "cost =  [0.01226847]\n",
      "cost =  [0.01226516]\n",
      "cost =  [0.01226187]\n",
      "cost =  [0.0122586]\n",
      "cost =  [0.01225537]\n",
      "cost =  [0.01225216]\n",
      "cost =  [0.01224898]\n",
      "cost =  [0.01224581]\n",
      "cost =  [0.01224268]\n",
      "cost =  [0.01223956]\n",
      "cost =  [0.01223647]\n",
      "cost =  [0.0122334]\n",
      "cost =  [0.01223035]\n",
      "cost =  [0.01222732]\n",
      "cost =  [0.01222431]\n",
      "cost =  [0.01222132]\n",
      "cost =  [0.01221835]\n",
      "cost =  [0.0122154]\n",
      "cost =  [0.01221247]\n",
      "cost =  [0.01220955]\n",
      "cost =  [0.01220665]\n",
      "cost =  [0.01220376]\n",
      "cost =  [0.0122009]\n",
      "cost =  [0.01219804]\n",
      "cost =  [0.01219521]\n",
      "cost =  [0.01219238]\n",
      "cost =  [0.01218957]\n",
      "cost =  [0.01218678]\n",
      "cost =  [0.012184]\n",
      "cost =  [0.01218123]\n",
      "cost =  [0.01217848]\n",
      "cost =  [0.01217574]\n",
      "cost =  [0.01217301]\n",
      "cost =  [0.01217029]\n",
      "cost =  [0.01216758]\n",
      "cost =  [0.01216489]\n",
      "cost =  [0.01216221]\n",
      "cost =  [0.01215954]\n",
      "cost =  [0.01215688]\n",
      "cost =  [0.01215423]\n",
      "cost =  [0.01215159]\n",
      "cost =  [0.01214896]\n",
      "cost =  [0.01214634]\n",
      "cost =  [0.01214372]\n",
      "cost =  [0.01214112]\n",
      "cost =  [0.01213853]\n",
      "cost =  [0.01213595]\n",
      "cost =  [0.01213337]\n",
      "cost =  [0.01213081]\n",
      "cost =  [0.01212825]\n",
      "cost =  [0.0121257]\n",
      "cost =  [0.01212316]\n",
      "cost =  [0.01212063]\n",
      "cost =  [0.0121181]\n",
      "cost =  [0.01211558]\n",
      "cost =  [0.01211307]\n",
      "cost =  [0.01211057]\n",
      "cost =  [0.01210807]\n",
      "cost =  [0.01210558]\n",
      "cost =  [0.0121031]\n",
      "cost =  [0.01210062]\n",
      "cost =  [0.01209815]\n",
      "cost =  [0.01209569]\n",
      "cost =  [0.01209323]\n",
      "cost =  [0.01209078]\n",
      "cost =  [0.01208834]\n",
      "cost =  [0.0120859]\n",
      "cost =  [0.01208347]\n",
      "cost =  [0.01208104]\n",
      "cost =  [0.01207862]\n",
      "cost =  [0.0120762]\n",
      "cost =  [0.01207379]\n",
      "cost =  [0.01207139]\n",
      "cost =  [0.01206899]\n",
      "cost =  [0.01206659]\n",
      "cost =  [0.0120642]\n",
      "cost =  [0.01206182]\n",
      "cost =  [0.01205944]\n",
      "cost =  [0.01205706]\n",
      "cost =  [0.01205469]\n",
      "cost =  [0.01205233]\n",
      "cost =  [0.01204996]\n",
      "cost =  [0.01204761]\n",
      "cost =  [0.01204525]\n",
      "cost =  [0.01204291]\n",
      "cost =  [0.01204056]\n",
      "cost =  [0.01203822]\n",
      "cost =  [0.01203589]\n",
      "cost =  [0.01203356]\n",
      "cost =  [0.01203123]\n",
      "cost =  [0.01202891]\n",
      "cost =  [0.01202659]\n",
      "cost =  [0.01202427]\n",
      "cost =  [0.01202196]\n",
      "cost =  [0.01201965]\n",
      "cost =  [0.01201735]\n",
      "cost =  [0.01201505]\n",
      "cost =  [0.01201275]\n",
      "cost =  [0.01201046]\n",
      "cost =  [0.01200817]\n",
      "cost =  [0.01200588]\n",
      "cost =  [0.0120036]\n",
      "cost =  [0.01200132]\n",
      "Final Cost= [0.01200001]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 8 is done\n",
      "Training for class 9 is in progress\n",
      "Cost =  [0.44991653]\n",
      "cost =  [0.03447571]\n",
      "cost =  [0.02068848]\n",
      "cost =  [0.01507631]\n",
      "cost =  [0.01251144]\n",
      "Final Cost= [0.01200068]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 9 is done\n",
      "All Weights calculated\n",
      "Weight vector list contains 10 weight vectors\n"
     ]
    }
   ],
   "source": [
    "list_weight_vectors=[]\n",
    "for unique_class in unique_classes:\n",
    "    print(\"Training for class {} is in progress\".format(unique_class))\n",
    "    y_modified=[0]*no_of_samples\n",
    "    for idx,value in enumerate(outputs):\n",
    "        if(value==unique_class):\n",
    "            y_modified[idx]=1\n",
    "    dataset['y_modified']=y_modified\n",
    "    weight_vector=logistic_regression(dataset)\n",
    "    list_weight_vectors.append(weight_vector)\n",
    "    dataset.drop(['y_modified'],axis=1)\n",
    "    print(\"Training for class {} is done\".format(unique_class))\n",
    "\n",
    "print(\"All Weights calculated\")\n",
    "print(\"Weight vector list contains {} weight vectors\".format(len(list_weight_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.85993904,  1.        ,  0.89490281,  0.19238635, -0.27122501,\n",
       "         -0.61506021, -0.21495017,  0.50324688,  0.88257173,  0.99916946,\n",
       "          0.56034083, -0.14636991, -0.41323418, -0.22144252,  0.19352458,\n",
       "          0.4363132 ,  0.88666905,  0.99960006,  0.56354119, -0.02128449,\n",
       "          0.00441043, -0.15854505,  0.34008184,  0.53154876,  0.95738324,\n",
       "          0.99985616,  0.70725818, -0.11860695, -0.43463401, -0.73344567,\n",
       "         -0.00875974,  0.59540129,  0.99723238,  1.        ,  0.69314566,\n",
       "          0.21778742, -0.45555066, -0.80891983, -0.25514303,  0.63425706,\n",
       "          1.        ,  0.99000194,  0.6250791 ,  0.49030595, -0.29325059,\n",
       "         -0.45996204,  0.0747053 ,  0.5563748 ,  0.99448543,  0.9866134 ,\n",
       "          0.77574576,  0.32973538, -0.04215185, -0.13124966,  0.15148763,\n",
       "          0.3562245 ,  0.95683584,  0.99991912,  0.89187813,  0.1061932 ,\n",
       "         -0.30858264, -0.44423685, -0.03883386,  0.57124831,  0.93174035]]),\n",
       " array([[ 0.79741438,  1.        ,  0.73637783, -0.09178063,  0.02138315,\n",
       "         -0.71639776,  0.03004888,  0.03698445,  0.60141525,  0.99834538,\n",
       "         -0.11516608, -0.64245146, -0.17945335, -0.03267534,  0.27260017,\n",
       "         -0.25548132,  0.70952048,  1.00493281,  0.18510573,  0.15388182,\n",
       "          0.71895624,  0.67254941, -0.04069844, -0.08177623,  0.89771434,\n",
       "          1.00186045,  0.33546345, -0.16093915,  0.2477026 , -0.11178765,\n",
       "          0.07614125, -0.21598267,  0.99112953,  1.        ,  0.106769  ,\n",
       "         -0.08175544, -0.08882182, -0.06157941, -0.41783449, -0.21070125,\n",
       "          1.        ,  0.98236273, -0.18429322, -0.14219525,  0.02937252,\n",
       "          0.28490481, -0.41064094, -0.09908158,  0.979978  ,  0.94702339,\n",
       "          0.29323729, -0.04862822,  0.07617879,  0.06451672,  0.02001326,\n",
       "         -0.29688384,  0.83361771,  0.99233504,  0.68950735, -0.40395025,\n",
       "         -0.26371886, -0.2030839 ,  0.12369921, -0.2404782 ,  0.60166007]]),\n",
       " array([[ 0.86067268,  1.        ,  0.86086555,  0.2520958 , -0.32313014,\n",
       "         -0.43894072, -0.14818773,  0.38651262,  0.83508766,  0.99755992,\n",
       "          0.64725898, -0.02033435, -0.47161238, -0.01664187, -0.03332629,\n",
       "          0.43170524,  0.89141008,  0.99824703,  0.46306455, -0.26574653,\n",
       "         -0.23213632,  0.42906649,  0.12426564,  0.51291285,  0.96557986,\n",
       "          0.99985515,  0.41079619, -0.87429176, -0.65579671, -0.03725574,\n",
       "         -0.16813962,  0.30677208,  0.99750727,  1.        ,  0.41289793,\n",
       "         -0.62843701, -0.19076828, -0.19270132, -0.6809612 ,  0.18125807,\n",
       "          1.        ,  0.99361224,  0.58525299,  0.06664207,  0.58647769,\n",
       "         -0.08037162, -0.51477473,  0.2308177 ,  0.99513231,  0.98542674,\n",
       "          0.85324002,  0.22630649,  0.3020381 ,  0.14338001,  0.17965262,\n",
       "          0.49807245,  0.90699227,  1.00050234,  0.86647752,  0.23925035,\n",
       "         -0.27525883, -0.28981909,  0.28033781,  0.72138589,  0.79726472]]),\n",
       " array([[ 0.85414174,  1.        ,  0.78492514,  0.03025858, -0.22734909,\n",
       "          0.18599803,  0.16351571,  0.1731804 ,  0.72375527,  0.98776549,\n",
       "          0.34215707,  0.16689993, -0.39776471, -0.07426724,  0.37491205,\n",
       "          0.33348043,  0.76654203,  0.99967473,  0.19212273, -0.60363546,\n",
       "         -0.32211977,  0.40483067, -0.02932047,  0.12964497,  0.91841548,\n",
       "          0.99985742,  0.38357625, -0.87014886, -0.04830361,  0.08483803,\n",
       "         -0.74757906, -0.19139732,  0.99816814,  1.        ,  0.36715005,\n",
       "         -0.46023188, -0.16107509,  0.23569883, -0.09659252,  0.13506847,\n",
       "          1.        ,  0.99520244,  0.56071406, -0.30462604, -0.65632697,\n",
       "         -0.08048339,  0.53437178,  0.68963796,  0.98380086,  0.99065647,\n",
       "          0.78950136, -0.08962227, -0.33758003, -0.10835525,  0.18965256,\n",
       "          0.26931801,  0.81584853,  0.99991964,  0.78366192,  0.11102649,\n",
       "         -0.09080503, -0.19088535, -0.14633111,  0.02069308,  0.58589415]]),\n",
       " array([[ 0.86653287,  1.        ,  0.92165475,  0.10822934, -0.58920076,\n",
       "         -0.54124538, -0.31617475,  0.37331093,  0.85260973,  0.99921375,\n",
       "          0.62633123, -0.51773537, -0.49391257, -0.48484004, -0.43905686,\n",
       "          0.32706487,  0.88573465,  0.99961904,  0.56399857, -0.11541754,\n",
       "          0.29387573,  0.13103548, -0.09859172,  0.5368686 ,  0.97605382,\n",
       "          1.00101331,  0.82054943,  0.10373813, -0.20293174, -0.19087434,\n",
       "          0.04479279,  0.68233069,  1.00000003,  1.        ,  0.94566162,\n",
       "          0.27332597,  0.02273254,  0.01155805, -0.00564211,  0.6224339 ,\n",
       "          1.        ,  1.00000867,  0.87881445,  0.16002961,  0.36166411,\n",
       "          0.38561728, -0.03701743,  0.37789714,  0.9829985 ,  0.99975483,\n",
       "          0.89104205, -0.24274594, -0.21916043,  0.05345998, -0.37866242,\n",
       "          0.24652661,  0.92254415,  0.99991827,  0.92136784,  0.04076783,\n",
       "         -0.53653663, -0.43110633, -0.24979292,  0.51951619,  0.91952194]]),\n",
       " array([[ 0.85876774,  1.        ,  0.9078315 ,  0.50702388, -0.29816725,\n",
       "         -0.20256315,  0.54204038,  0.54694227,  0.68856195,  0.99918478,\n",
       "          0.5589766 ,  0.13200421, -0.43657473, -0.24902438, -0.22515903,\n",
       "          0.19937496,  0.71060183,  0.9995552 ,  0.49285341,  0.26727877,\n",
       "          0.02003757, -0.38810878, -0.85945835, -0.14817338,  0.89298343,\n",
       "          0.99984839,  0.77003025,  0.34966219,  0.27893419, -0.02488355,\n",
       "         -0.48985393, -0.06974727,  0.99440861,  1.        ,  0.50457798,\n",
       "          0.10999625, -0.12243678, -0.14864148, -0.17429103,  0.33894088,\n",
       "          1.        ,  0.99309306,  0.45361182, -0.44061177, -0.38770881,\n",
       "         -0.17083347,  0.22959141,  0.48859844,  0.9951336 ,  0.98360639,\n",
       "          0.7786971 , -0.25636729, -0.28623741, -0.06439971, -0.01622905,\n",
       "          0.13116175,  0.92159875,  0.99992028,  0.90800278,  0.47841284,\n",
       "         -0.16987533, -0.57366931, -0.40910155,  0.23700225,  0.76859285]]),\n",
       " array([[ 8.53776439e-01,  1.00000000e+00,  8.42230827e-01,\n",
       "         -2.19838667e-01, -3.89356591e-01, -4.85013608e-01,\n",
       "         -7.82598300e-02,  5.57491074e-01,  8.97619111e-01,\n",
       "          9.99169091e-01,  4.01717834e-01, -4.78572253e-01,\n",
       "         -4.03316470e-01, -6.10979711e-01, -3.91773033e-01,\n",
       "          4.38442926e-01,  8.87530824e-01,  9.99607870e-01,\n",
       "          3.20634077e-01,  6.34345618e-02,  1.13109242e-01,\n",
       "         -3.36546886e-01, -5.42005861e-01,  2.60675712e-01,\n",
       "          9.35748525e-01,  9.99849399e-01,  5.24059560e-01,\n",
       "          8.48367164e-02, -4.36359196e-02, -3.09528747e-01,\n",
       "         -5.01043203e-01,  1.84362614e-02,  9.94731780e-01,\n",
       "          1.00000000e+00,  4.92639334e-01,  4.00916977e-01,\n",
       "          2.66110257e-01, -3.62596963e-04, -1.63358394e-01,\n",
       "          2.45679063e-01,  1.00000000e+00,  9.83425000e-01,\n",
       "          3.31342324e-01,  5.85266267e-01,  1.63470505e-01,\n",
       "         -3.33179298e-01,  1.43293720e-01,  7.95926974e-01,\n",
       "          1.00457056e+00,  9.79828273e-01,  6.38177341e-01,\n",
       "          1.31380831e-01,  2.25350742e-01, -6.24503272e-01,\n",
       "          2.23167156e-01,  6.83751628e-01,  8.61578328e-01,\n",
       "          9.99919919e-01,  8.38250198e-01, -2.15777894e-01,\n",
       "         -4.71231820e-01, -4.47940480e-01,  3.45719113e-01,\n",
       "          4.71919525e-01,  7.74149075e-01]]),\n",
       " array([[ 0.86392421,  1.        ,  0.85717499,  0.19658423, -0.2215293 ,\n",
       "         -0.28509933,  0.28268445,  0.64609186,  0.93421399,  0.99694124,\n",
       "          0.5113332 , -0.13422881, -0.41174034, -0.10052089,  0.20933328,\n",
       "          0.50517676,  0.93209121,  0.99959654,  0.40349905, -0.59305393,\n",
       "         -0.27760861,  0.14736268,  0.18941187,  0.51653862,  0.96571605,\n",
       "          0.99985408,  0.40700446, -0.69878877, -0.34780352,  0.01581856,\n",
       "          0.21533446,  0.58976245,  0.99641643,  1.        ,  0.44410186,\n",
       "         -0.05334698,  0.19107941,  0.15244084,  0.08282745,  0.7107706 ,\n",
       "          1.        ,  0.99351169,  0.53453832, -0.03784133,  0.28488691,\n",
       "          0.22648595, -0.31989973,  0.4276629 ,  0.99478816,  0.99122624,\n",
       "          0.7702785 , -0.28780321,  0.0738107 , -0.49236698, -0.65750714,\n",
       "          0.31483244,  0.95953289,  0.99991636,  0.84986935,  0.16943974,\n",
       "         -0.32290363, -1.00518415, -0.36082635,  0.52125836,  0.92820535]]),\n",
       " array([[-0.38375156,  1.        ,  0.05030635,  0.23895376, -0.21736048,\n",
       "         -0.01951375, -0.0196507 , -0.62849864,  0.0080163 ,  1.32147503,\n",
       "         -0.08040927,  0.13372108,  0.00616856, -0.18080558,  0.14172304,\n",
       "          0.18073076, -0.10583181,  0.61126094,  0.11924844,  0.16940052,\n",
       "          0.03994008,  0.01402916,  0.0911396 ,  0.12445091,  0.35056314,\n",
       "          0.73114631, -0.1752071 , -0.05663596,  0.14271262, -0.08995397,\n",
       "         -0.00944979, -0.16539876,  0.95376886,  1.        , -0.44832718,\n",
       "         -0.11966256,  0.27289712, -0.02350801, -0.07421435, -1.05201178,\n",
       "          1.        ,  0.97415225,  0.18933372,  0.13325276,  0.01562932,\n",
       "          0.03808033,  0.05291197,  0.00222024,  0.81251195,  0.78145548,\n",
       "         -0.13815091,  0.16986163, -0.24893756, -0.20554474,  0.0987413 ,\n",
       "          0.03152686, -0.17036836,  0.93602731, -0.00753737, -0.57613475,\n",
       "         -0.03756236,  0.02968524, -0.22212268, -0.19484548,  0.12951746]]),\n",
       " array([[ 0.84140842,  1.        ,  0.77166807, -0.09407296, -0.12335756,\n",
       "         -0.50563519, -0.4029266 ,  0.09578601,  0.6029823 ,  0.99390746,\n",
       "          0.23284306,  0.29407496, -0.21755446, -0.22105325,  0.16259262,\n",
       "          0.14110748,  0.64121029,  0.99556195,  0.28463194,  0.31069853,\n",
       "         -0.09002555,  0.01455814,  0.80070097,  0.14011515,  0.83088129,\n",
       "          0.99985714,  0.25386764, -0.00245731,  0.35988756,  0.20660627,\n",
       "          0.43455601,  0.03214631,  0.98270592,  1.        ,  0.04475017,\n",
       "         -0.47932407,  0.2521489 , -0.36335248, -0.29910164,  0.24367683,\n",
       "          1.        ,  0.99459823,  0.4236191 , -0.68334243, -0.62512596,\n",
       "         -0.38322671,  0.02626556,  0.1585584 ,  0.99580413,  0.98952607,\n",
       "          0.80811624, -0.11927942, -0.17963442, -0.22755717, -0.0915444 ,\n",
       "          0.11724992,  0.91587321,  0.99991993,  0.81093438, -0.08628349,\n",
       "         -0.23710554, -0.28379697, -0.32138194,  0.21566011,  0.78396365]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_weight_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outputs are  [0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 5, 8, 9, 8, 4, 1, 7, 7, 3, 1, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 1, 8, 1, 5, 0, 9, 8, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 1, 6, 3, 1, 3, 8, 1, 7, 1, 8, 4, 3, 1, 4, 0, 5, 3, 8, 9, 1, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 8, 4, 8, 1, 4, 9, 0, 8, 9, 1, 0, 1, 2, 3, 1, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 1, 4, 3, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 8, 4, 4, 7, 2, 1, 2, 8, 5, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 8, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 8, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 8, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 8, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 8, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 8, 7, 8, 8, 4, 6, 8, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 8, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 8, 1, 7, 5, 1, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 1, 9, 0, 8, 9, 8, 0, 1, 1, 9, 4, 5, 6, 8, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 8, 4, 5, 6, 7, 8, 9, 0, 8, 5, 5, 6, 5, 0, 9, 8, 8, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 8, 3, 3, 7, 8, 3, 4, 6, 6, 6, 4, 9, 1, 5, 8, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 8, 8, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 4, 5, 5, 6, 5, 0, 1, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 8, 1, 1, 8, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 1, 5, 8, 8, 8, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 7, 0, 5, 3, 6, 4, 6, 1, 7, 5, 4, 4, 7, 8, 8, 8, 2, 5, 7, 4, 5, 4, 8, 8, 7, 9, 0, 8, 1, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 5, 7, 1, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 2, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 8, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 2, 3, 4, 5, 6, 8, 8, 9, 0, 1, 2, 3, 4, 9, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 3, 7, 7, 3, 5, 1, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 9, 7, 9, 5, 4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 9, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 8, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 8, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 3, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 5, 9, 5, 4, 1, 7, 7, 7, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 5, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 8, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 0, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 8, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 9, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 8, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 8, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 0, 0, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 8, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 0, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 0, 4, 0, 5, 3, 6, 9, 6, 9, 8, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 3, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 9, 1, 9, 0, 1, 8, 3, 4, 5, 6, 9, 8, 1, 2, 3, 4, 5, 1, 8, 1, 9, 8, 9, 5, 5, 6, 5, 0, 9, 8, 5, 8, 4, 1, 7, 7, 3, 5, 1, 8, 0, 2, 2, 4, 8, 2, 0, 1, 2, 6, 8, 7, 7, 8, 3, 4, 6, 6, 6, 9, 9, 1, 5, 0, 9, 8, 2, 8, 0, 1, 7, 6, 3, 2, 1, 7, 9, 6, 3, 1, 8, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 8, 3, 8, 1, 7, 8, 4, 4, 7, 2, 2, 5, 7, 8, 5, 9, 4, 5, 0, 8, 8, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 8, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 7, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 8, 8, 7, 5, 8, 4, 8, 6, 6, 4, 9, 8, 5, 0, 9, 5, 2, 8, 2, 0, 0, 8, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 8, 1, 4, 0, 5, 3, 6, 9, 6, 8, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8]\n",
      "Actual outputs are  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 2, 5, 7, 9, 5, 4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "# Generate all predicted values\n",
    "\n",
    "import itertools\n",
    "predicted_outputs=[]\n",
    "\n",
    "def predict_output(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24,x25,x26,x27,x28,x29,x30,x31,x32,x33,x34,x35,x36,x37,x38,x39,x40,x41,x42,x43,x44,x45,x46,x47,x48,x49,x50,x51,x52,x53,x54,x55,x56,x57,x58,x59,x60,x61,x62,x63,x64,unique_class):\n",
    "    return (list_weight_vectors[unique_class][0][0]+\n",
    "            (list_weight_vectors[unique_class][0][1]*x1)+\n",
    "            (list_weight_vectors[unique_class][0][2]*x2)+\n",
    "            (list_weight_vectors[unique_class][0][3]*x3)+\n",
    "            (list_weight_vectors[unique_class][0][4]*x4)+\n",
    "            (list_weight_vectors[unique_class][0][5]*x5)+\n",
    "            (list_weight_vectors[unique_class][0][6]*x6)+\n",
    "            (list_weight_vectors[unique_class][0][7]*x7)+\n",
    "            (list_weight_vectors[unique_class][0][8]*x8)+\n",
    "            (list_weight_vectors[unique_class][0][9]*x9)+\n",
    "            (list_weight_vectors[unique_class][0][10]*x10)+\n",
    "            (list_weight_vectors[unique_class][0][11]*x11)+\n",
    "            (list_weight_vectors[unique_class][0][12]*x12)+\n",
    "            (list_weight_vectors[unique_class][0][13]*x13)+\n",
    "            (list_weight_vectors[unique_class][0][14]*x14)+\n",
    "            (list_weight_vectors[unique_class][0][15]*x15)+\n",
    "            (list_weight_vectors[unique_class][0][16]*x16)+\n",
    "            (list_weight_vectors[unique_class][0][17]*x17)+\n",
    "            (list_weight_vectors[unique_class][0][18]*x18)+\n",
    "            (list_weight_vectors[unique_class][0][19]*x19)+\n",
    "            (list_weight_vectors[unique_class][0][20]*x20)+\n",
    "            (list_weight_vectors[unique_class][0][21]*x21)+\n",
    "            (list_weight_vectors[unique_class][0][22]*x22)+\n",
    "            (list_weight_vectors[unique_class][0][23]*x23)+\n",
    "            (list_weight_vectors[unique_class][0][24]*x24)+\n",
    "            (list_weight_vectors[unique_class][0][25]*x25)+\n",
    "            (list_weight_vectors[unique_class][0][26]*x26)+\n",
    "            (list_weight_vectors[unique_class][0][27]*x27)+\n",
    "            (list_weight_vectors[unique_class][0][28]*x28)+\n",
    "            (list_weight_vectors[unique_class][0][29]*x29)+\n",
    "            (list_weight_vectors[unique_class][0][30]*x30)+\n",
    "            (list_weight_vectors[unique_class][0][31]*x31)+\n",
    "            (list_weight_vectors[unique_class][0][32]*x32)+\n",
    "            (list_weight_vectors[unique_class][0][33]*x33)+\n",
    "            (list_weight_vectors[unique_class][0][34]*x34)+\n",
    "            (list_weight_vectors[unique_class][0][35]*x35)+\n",
    "            (list_weight_vectors[unique_class][0][36]*x36)+\n",
    "            (list_weight_vectors[unique_class][0][37]*x37)+\n",
    "            (list_weight_vectors[unique_class][0][38]*x38)+\n",
    "            (list_weight_vectors[unique_class][0][39]*x39)+\n",
    "            \n",
    "            (list_weight_vectors[unique_class][0][40]*x40)+\n",
    "            (list_weight_vectors[unique_class][0][41]*x41)+\n",
    "            (list_weight_vectors[unique_class][0][42]*x42)+\n",
    "            (list_weight_vectors[unique_class][0][43]*x43)+\n",
    "            (list_weight_vectors[unique_class][0][44]*x44)+\n",
    "            (list_weight_vectors[unique_class][0][45]*x45)+\n",
    "            (list_weight_vectors[unique_class][0][46]*x46)+\n",
    "            (list_weight_vectors[unique_class][0][47]*x47)+\n",
    "            (list_weight_vectors[unique_class][0][48]*x48)+\n",
    "            (list_weight_vectors[unique_class][0][49]*x49)+\n",
    "        \n",
    "            (list_weight_vectors[unique_class][0][50]*x50)+\n",
    "            (list_weight_vectors[unique_class][0][51]*x51)+\n",
    "            (list_weight_vectors[unique_class][0][52]*x52)+\n",
    "            (list_weight_vectors[unique_class][0][53]*x53)+\n",
    "            (list_weight_vectors[unique_class][0][54]*x54)+\n",
    "            (list_weight_vectors[unique_class][0][55]*x55)+\n",
    "            (list_weight_vectors[unique_class][0][56]*x56)+\n",
    "            (list_weight_vectors[unique_class][0][57]*x57)+\n",
    "            (list_weight_vectors[unique_class][0][58]*x58)+\n",
    "            (list_weight_vectors[unique_class][0][59]*x59)+\n",
    "        \n",
    "            (list_weight_vectors[unique_class][0][60]*x60)+\n",
    "            (list_weight_vectors[unique_class][0][61]*x61)+\n",
    "            (list_weight_vectors[unique_class][0][62]*x62)+\n",
    "            (list_weight_vectors[unique_class][0][63]*x63)+\n",
    "            (list_weight_vectors[unique_class][0][64]*x64))\n",
    "            \n",
    "predicted_outputs=[]\n",
    "for xa1,xa2,xa3,xa4,xa5,xa6,xa7,xa8,xa9,xa10,xa11,xa12,xa13,xa14,xa15,xa16,xa17,xa18,xa19,xa20,xa21,xa22,xa23,xa24,xa25,xa26,xa27,xa28,xa29,xa30,xa31,xa32,xa33,xa34,xa35,xa36,xa37,xa38,xa39,xa40,xa41,xa42,xa43,xa44,xa45,xa46,xa47,xa48,xa49,xa50,xa51,xa52,xa53,xa54,xa55,xa56,xa57,xa58,xa59,xa60,xa61,xa62,xa63,xa64 in itertools.zip_longest(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24,x25,x26,x27,x28,x29,x30,x31,x32,x33,x34,x35,x36,x37,x38,x39,x40,x41,x42,x43,x44,x45,x46,x47,x48,x49,x50,x51,x52,x53,x54,x55,x56,x57,x58,x59,x60,x61,x62,x63,x64):\n",
    "    probability_classes=[]\n",
    "    for unique_class in unique_classes:                \n",
    "        predicted_output=predict_output(xa1,xa2,xa3,xa4,xa5,xa6,xa7,xa8,xa9,xa10,xa11,xa12,xa13,xa14,xa15,xa16,xa17,xa18,xa19,xa20,xa21,xa22,xa23,xa24,xa25,xa26,xa27,xa28,xa29,xa30,xa31,xa32,xa33,xa34,xa35,xa36,xa37,xa38,xa39,xa40,xa41,xa42,xa43,xa44,xa45,xa46,xa47,xa48,xa49,xa50,xa51,xa52,xa53,xa54,xa55,xa56,xa57,xa58,xa59,xa60,xa61,xa62,xa63,xa64,unique_class)\n",
    "        probability = 1/(1+(np.exp(-predicted_output)))\n",
    "        probability_classes.append(probability)   \n",
    "    \n",
    "    max_probability=max(probability_classes)\n",
    "    predicted_output=probability_classes.index(max_probability)\n",
    "    predicted_outputs.append(predicted_output)\n",
    "\n",
    "print(\"Predicted outputs are \",predicted_outputs)\n",
    "print(\"Actual outputs are \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model is 92.93266555370062% accurate\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy:\n",
    "\n",
    "true_count=0\n",
    "total_outputs=m\n",
    "for predicted_value, actual_value in itertools.zip_longest(predicted_outputs,y):\n",
    "    if(predicted_value==actual_value):\n",
    "        true_count=true_count+1\n",
    "    \n",
    "accuracy= (true_count/m)*100\n",
    "\n",
    "print(\"The Model is {}% accurate\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
